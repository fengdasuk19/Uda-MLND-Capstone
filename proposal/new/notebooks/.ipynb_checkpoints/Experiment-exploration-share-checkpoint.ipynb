{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if ('modeling.document' == phase) and ('Word2Vec' == idpart['model.word']):\n",
    "    if 'ArithmeticMean' == idpart['model.document']:\n",
    "        w2v_tag = 'w2v.ArithmeticMean'\n",
    "        count_clusters=vecsize\n",
    "    else:\n",
    "        w2v_tag = 'w2v.{}'.format(idpart['model.document'])\n",
    "        # KMeans(modified version, using cosine distance)\n",
    "        from sklearn.cluster import KMeans\n",
    "    \n",
    "        w2v_raw = []\n",
    "    \n",
    "        for vocab in model.wv.vocab:\n",
    "            w2v_raw.append(model[vocab])\n",
    "    \n",
    "        w2v_raw = np.array(w2v_raw)\n",
    "    \n",
    "        count_clusters=196\n",
    "        clus = KMeans(n_clusters=count_clusters, random_state=19, n_jobs=-1)\n",
    "        clus.fit(w2v_raw)\n",
    "    \n",
    "    # Read data and save it in data['vec.train'] 和 data['vec.test']\n",
    "    data = {}\n",
    "    data['vec.train'] = {w2v_tag:[], 'class':[]}\n",
    "    data['vec.test'] = {w2v_tag:[], 'class':[]}\n",
    "    \n",
    "    for tpart in ['train', 'test']:\n",
    "        dirpath = paths['dir.{}'.format(tpart)]\n",
    "        for (ind, cls) in enumerate(os.listdir(dirpath)):\n",
    "            clspath = os.path.join(dirpath, cls)\n",
    "            files = os.listdir(clspath)\n",
    "            for f in files:\n",
    "                fpath = os.path.join(clspath, f)\n",
    "                # Start to represent a document\n",
    "                with open(fpath, 'r') as readf:\n",
    "                    tokens = [token for token in readf.read().split()]\n",
    "                    if 'ArithmeticMean' == idpart['model.document']:\n",
    "                        # Word2Vec representation: Arithmetic Mean\n",
    "                        # begin\n",
    "                        vec = np.array([0.0 for i in range(vecsize)])\n",
    "                        expectationVal = np.array([0.0 for i in range(vecsize)])\n",
    "                        countvec = 0\n",
    "                        for token in tokens:\n",
    "                            try:\n",
    "                                vec += model[token]\n",
    "                                countvec += 1\n",
    "                            except:\n",
    "                                vec += expectationVal\n",
    "                        vec = vec / float(countvec)#float(len(tokens))\n",
    "                         # end\n",
    "                    else:\n",
    "                        # Word2Vec representation: Bag-of-Concepts\n",
    "                        # begin\n",
    "                        vec = np.array([0.0 for i in range(count_clusters + 1)])\n",
    "                        expectationVal = np.array([0.0 for i in range(count_clusters + 1)])\n",
    "                        countvec = 0\n",
    "                        for token in tokens:\n",
    "                            try:\n",
    "                                whichCenter = clus.predict([model[token]])[0] \n",
    "                                vec[whichCenter] = 1 + vec[whichCenter]\n",
    "                            except:\n",
    "                                vec[count_clusters] = 1 + vec[count_clusters] # count for unknown word\n",
    "                         # end\n",
    "                # End of attempt to represent a document\n",
    "                # Save in dictionary\n",
    "                if 'ArithmeticMean' == idpart['model.document']:\n",
    "                    vec = vec\n",
    "                else:\n",
    "                    vec = vec[:-1]\n",
    "                data['vec.{}'.format(tpart)][w2v_tag].append(vec)\n",
    "                data['vec.{}'.format(tpart)]['class'].append(cls)\n",
    "    \n",
    "        tmp = data['vec.{}'.format(tpart)]\n",
    "        ind = (random.sample(range(len(tmp['class'])), 1))[0]\n",
    "#         print(\"sample(transformed) from {}[{}]:\\n[corpus]\\n {}\\n[class]\\n{}\".format(\n",
    "#                 tpart, \n",
    "#                 ind, \n",
    "#                 tmp[w2v_tag][ind], \n",
    "#                 tmp['class'][ind]\n",
    "#             )\n",
    "#         )\n",
    "#         print()\n",
    "    \n",
    "#     print(\"Read data and save it in data['vec.train'] 和 data['vec.test'] successfully.\")\n",
    "    \n",
    "    # Save in Pandas.DataFrame\n",
    "    # 将 data['matrix.train'] 与 data['matrix.test'] 转换成 Pandas.DataFrame 格式，保存到 df['train'] 和 df['test'] 中（df 为字典格式：String -> DataFrame）\n",
    "    df = {}\n",
    "    csvpath_root = os.path.join(paths['dir.dataroot'], 'data_CSV')\n",
    "    for tpart in ['train', 'test']:\n",
    "        datadict = {}\n",
    "        datadict['class'] = data['vec.{}'.format(tpart)]['class']\n",
    "        datavec = np.array(data['vec.{}'.format(tpart)][w2v_tag])\n",
    "        for col in range(count_clusters):#+1): #\n",
    "            datadict[col]= datavec[:, col]\n",
    "    \n",
    "        df[tpart] = pd.DataFrame(data=datadict)\n",
    "#         print(\"See df[{}]\".format(tpart))\n",
    "        display(df[tpart])\n",
    "#         print(\"\\n\\n\\n\")\n",
    "        # write data in DataFrame into CSV\n",
    "        csvpath = os.path.join(csvpath_root, '{}-{}-{}-{}-{}.csv'.format(tpart, idpart['model.source'], idpart['tool.word'], idpart['model.word'], idpart['model.document']))\n",
    "        df[tpart].to_csv(csvpath, columns=df[tpart].columns)\n",
    "    \n",
    "#     print(\"Step 3 Succeed.\")\n",
    "    \n",
    "    # 繁琐点：研究如何把 CSR 矩阵中的数据规整好放到 DataFrame 中，并与 Class 一一对应\n",
    "     \n",
    "    # if wanna read data from CSV file\n",
    "    df = {}\n",
    "    csvpath_root = os.path.join(paths['dir.dataroot'], 'data_CSV')\n",
    "    for tpart in ['train', 'test']:\n",
    "        csvpath = os.path.join(csvpath_root, '{}-{}-{}-{}-{}.csv'.format(tpart, idpart['model.source'], idpart['tool.word'], idpart['model.word'], idpart['model.document']))\n",
    "        if os.path.exists(csvpath):\n",
    "            df[tpart] = pd.DataFrame.from_csv(csvpath)\n",
    "            df[tpart] = df[tpart].sample(frac=1)\n",
    "            df[tpart].reset_index(drop=True, inplace=True)\n",
    "#             print(\"read {} successfully\".format(tpart))\n",
    "            display(df[tpart])\n",
    "            \n",
    "    # Store data in X_train, y_train, X_test, y_test\n",
    "    ##train\n",
    "    X_train, y_train = df['train'].drop('class', axis=1), df['train']['class']\n",
    "    ##test\n",
    "    X_test, y_test_true = df['test'].drop('class', axis=1), df['test']['class']\n",
    "    \n",
    "    # If or not transform into TFIDF\n",
    "    if 'BagOfConcepts-TFIDF' == idpart['model.document']:\n",
    "        from sklearn.feature_extraction.text import TfidfTransformer\n",
    "        tfidf = TfidfTransformer()\n",
    "        #train\n",
    "        X_train = tfidf.fit_transform(X_train)\n",
    "        #test\n",
    "        X_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练前制作 ID，保存数据\n",
    "\n",
    "if 'save_before_train' == phase:\n",
    "    ID = '{}.{}.{}.{}.{}'.format(idpart['model.source'], idpart['tool.word'], idpart['model.word'], idpart['model.document'], idpart['classifier'])\n",
    "    record_data[ID] = {}\n",
    "    for ilabel in idpart:\n",
    "        record_data[ilabel][ID] = idpart[ilabel]\n",
    "\n",
    "    for labelpart in ['raw', 'format']:\n",
    "        for ilabel in ['model', 'train', 'evaluate', 'all']:\n",
    "            record_data['time.{}.{}'.format(labelpart, ilabel)][ID] = {} if 'raw' == labelpart else 0.0\n",
    "\n",
    "    if 'SVM' == idpart['classifier']:\n",
    "        record_data['time.raw.model'][ID] = timev\n",
    "    else:\n",
    "        ID_SVM = '{}.{}.{}.{}.{}'.format(idpart['model.source'], idpart['tool.word'], idpart['model.word'], idpart['model.document'], 'SVM')\n",
    "        record_data['time.raw.model'][ID] = record_data['time.raw.model'][ID_SVM]\n",
    "#         for timepart, timesvm in record_data['time.raw.model'][ID_SVM].items():\n",
    "#             record_data['time.raw.model'][ID][timepart] = timesvm + timev[timepart]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 实验后处理：显示数据\n",
    "\n",
    "if 'post-display' == phase:\n",
    "    # sum of time\n",
    "\n",
    "    for i in timeDict.keys():\n",
    "        record_data['time.raw.all'][ID][i] = sum([record_data['time.raw.{}'.format(timepart)][ID][i] for timepart in ['model', 'train', 'evaluate']])\n",
    "\n",
    "    # formation of time\n",
    "\n",
    "    for timepart in ['model', 'train', 'evaluate', 'all']:\n",
    "            record_data['time.format.{}'.format(timepart)][ID] = timeInFormat(record_data['time.raw.{}'.format(timepart)][ID])\n",
    "\n",
    "    print(record_data)\n",
    "\n",
    "    records_df_raw = pd.DataFrame(\n",
    "        data=record_data, \n",
    "        columns=record_labels\n",
    "    )\n",
    "\n",
    "    # remove raw time\n",
    "    records_df_display = records_df_raw.drop(record_labels[-4:], axis=1)\n",
    "\n",
    "    # use reset_index(drop=True) to use integers instead of ID as the indices\n",
    "    # (data can be distinguish using each column, so ID is dispensable)\n",
    "    records_df_display = records_df_display.reset_index(drop=True)\n",
    "\n",
    "    display(records_df_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot representation for labels\n",
    "# 标签独热向量化\n",
    "\n",
    "if 'one-hot-labels' == phase:\n",
    "    csvpath_root = os.path.join(paths['dir.dataroot'], 'data_CSV')\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(df['train']['class'])\n",
    "\n",
    "    df_new = {}\n",
    "    for tpart in ['train', 'test']:\n",
    "        labels = lb.transform(df[tpart]['class'])\n",
    "        labelsDf = pd.DataFrame(labels, columns=[\"class-{}\".format(i) for i in range(len(lb.classes_))])\n",
    "        df_new[tpart] = {}\n",
    "        df_new[tpart]['y'] = labelsDf\n",
    "        df_new[tpart]['x'] = df[tpart].drop('class', axis=1)\n",
    "        df_new[tpart]['all'] = df_new[tpart]['x'].join(df_new[tpart]['y'])\n",
    "        #save in CSV\n",
    "        for subpart in ['x', 'y', 'all']:\n",
    "            csvpath = os.path.join(csvpath_root, '{}-cleanLabels{}-{}-{}-{}.csv'.format(tpart, idpart['model.source'], idpart['tool.word'], idpart['model.word'], idpart['model.document']))\n",
    "            df_new[tpart][subpart].to_csv(csvpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN train\n",
    "# 训练分类器：DNN\n",
    "if 'DNN-train' == phase: \n",
    "    COL_OUTCOME = 'class'\n",
    "    COL_FEATURE = [str(col) for col in list(df['train'].columns) if col != COL_OUTCOME]\n",
    "\n",
    "    cls2num = {cls:ind for (ind, cls) in enumerate(df['train']['class'].unique())}\n",
    "\n",
    "    def my_input_fn(dataset):\n",
    "        # Save dataset in tf format\n",
    "        feature_cols = {\n",
    "            str(col): tf.constant(\n",
    "                df[dataset][str(col)].values\n",
    "            )\n",
    "            for col in COL_FEATURE\n",
    "        }\n",
    "        labels = tf.constant([cls2num[labelname] for labelname in df[dataset][COL_OUTCOME].values])\n",
    "        # Returns the feature columns and labels in tf format\n",
    "        return feature_cols, labels\n",
    "\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(column_name=str(col)) for col in COL_FEATURE]\n",
    "    clf = tf.contrib.learn.DNNClassifier(\n",
    "        feature_columns=feature_columns, \n",
    "        hidden_units=[512], \n",
    "        n_classes=len(df['train']['class'].unique())\n",
    "    )\n",
    "\n",
    "    clf.fit(input_fn=lambda: my_input_fn('train'), steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN evaluate\n",
    "# 评估分类器：DNN\n",
    "if 'DNN-evaluate' == phase: \n",
    "    X_tensor_test, yt = my_input_fn('test')\n",
    "    tensorPredCls = list(clf.predict(input_fn=lambda: my_input_fn('test')))\n",
    "    num2cls = {v:k for (k, v) in cls2num.items()}\n",
    "    tensorPredClsStr = [num2cls[i] for i in tensorPredCls]\n",
    "    y_test_true = df['test']['class']\n",
    "\n",
    "    record_data['accuracy'][ID] = accuracy_score(y_test_true, tensorPredClsStr)\n",
    "    record_data['macro-F1'][ID] = f1_score(y_test_true, tensorPredClsStr, average='macro')\n",
    "    record_data['micro-F1'][ID] = f1_score(y_test_true, tensorPredClsStr, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN-init\n",
    "# 训练前初始化：CNN\n",
    "if 'CNN-init' == phase:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    COL_OUTCOME = 'class'\n",
    "    COL_FEATURE = [col for col in list(df['train'].columns) if col != COL_OUTCOME]\n",
    "\n",
    "    # cls2num = {cls:ind for (ind, cls) in enumerate(df['train']['class'].unique())}\n",
    "\n",
    "    count_feature = len(COL_FEATURE)\n",
    "    count_class = len(df['train']['class'].unique())\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, count_clusters], name='x')\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, count_class], name='y_')\n",
    "\n",
    "    W = tf.Variable(tf.zeros([count_feature, count_class]), name='W')\n",
    "    b = tf.Variable(tf.zeros([count_class]), name='b')\n",
    "    y = tf.matmul(x, W, name='y') + b\n",
    "\n",
    "    # cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "    def weight_variable(shape, name=None):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name)\n",
    "\n",
    "    def bias_variable(shape, name=None):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name)\n",
    "\n",
    "    def conv2d(x, W, name=None):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool_2x2(x, name=None):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32], name='W_conv1')\n",
    "    b_conv1 = bias_variable([32], name='b_conv1')\n",
    "    \n",
    "    if 'ArithmeticMean' == idpart['model.document']:\n",
    "        x_text = tf.reshape(x, [-1, 28, 28, 1], name='x_text')\n",
    "    else:\n",
    "        x_text = tf.reshape(x, [-1, 14, 14, 1], name='x_text')\n",
    "        \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_text, W_conv1) + b_conv1, name='h_conv1')\n",
    "    h_pool1 = max_pool_2x2(h_conv1, name='h_pool1')\n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64], name='W_conv2')\n",
    "    b_conv2 = bias_variable([64], name='b_conv2')\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, name='h_conv2')\n",
    "    h_pool2 = max_pool_2x2(h_conv2, name='h_pool2')\n",
    "\n",
    "    if 'ArithmeticMean' == idpart['model.document']:    \n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024], name='W_fc1')\n",
    "    else:\n",
    "        W_fc1 = weight_variable([4 * 4 * 64, 1024], name='W_fc1')\n",
    "    b_fc1 = bias_variable([1024], name='b_fc1')\n",
    "\n",
    "    if 'ArithmeticMean' == idpart['model.document']:\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64], name='h_pool2_flat')\n",
    "    else:\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 4 * 4 * 64], name='h_pool2_flat')\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name='h_fc1')\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name='h_fc1_drop')\n",
    "\n",
    "    W_fc2 = weight_variable([1024, count_class], name='W_fc2')\n",
    "    b_fc2 = bias_variable([count_class], name='b_fc2')\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2, name='y_conv') + b_fc2\n",
    "\n",
    "    # print(\"CNN initialization finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN train\n",
    "# 训练分类器：CNN\n",
    "if 'CNN-train' == phase: \n",
    "    ### Start to train and evaluate the model\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv), name='cross_entropy')\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    # correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    x_input = df_new['train']['x']\n",
    "    x_input = [np.array([\n",
    "                np.float32(x_input.iloc[i].values)\n",
    "            ])\n",
    "        for i in range(x_input.shape[0])]\n",
    "    y_input = df_new['train']['y']\n",
    "    y_input = [np.array([\n",
    "                np.float32(y_input.iloc[i].values)\n",
    "            ])\n",
    "        for i in range(y_input.shape[0])]\n",
    "    # y_input = [np.array([y_input.iloc[i].values]) for i in range(y_input.shape[0])]\n",
    "\n",
    "    # not use random input\n",
    "\n",
    "    batchsize = 50\n",
    "    for i in range(df['train'].shape[0] - batchsize):\n",
    "    #     if 0 == i % 100:\n",
    "    #         train_accuracy = []\n",
    "    #         for j in range(50):\n",
    "    #             train_accuracy.append(accuracy.eval(feed_dict={\n",
    "    #                     keep_prob: 1,\n",
    "    #                     x:  np.array([elem[0] for elem in x_input[i+j:i+j+50]]),#x_input.iloc[i+j].values, #\n",
    "    #                     y_: np.array([elem[0] for elem in y_input[i+j:i+j+50]])#y_input.iloc[i+j].values #\n",
    "    #                 })\n",
    "    #             )\n",
    "    #         print(\"step {}, training accuracy {}\".format(i, np.mean(train_accuracy)))\n",
    "        train_step.run(feed_dict={\n",
    "            keep_prob: 0.5,\n",
    "            x:  np.array([elem[0] for elem in x_input[i:i+batchsize]]),#x_input.iloc[i].values, #\n",
    "            y_: np.array([elem[0] for elem in y_input[i:i+batchsize]])#y_input.iloc[i].values#\n",
    "        })\n",
    "\n",
    "    # print(\"CNN training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN evaluate\n",
    "# 评估分类器：CNN\n",
    "if 'CNN-evaluate' == phase: \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    # Evaluate\n",
    "\n",
    "    tfacc = []\n",
    "    predlist, truelist = [], []\n",
    "\n",
    "    x_input = df_new['test']['x']\n",
    "    x_input = [np.array([\n",
    "                np.float32(x_input.iloc[i].values)\n",
    "            ])\n",
    "        for i in range(x_input.shape[0])]\n",
    "    y_input = df_new['test']['y']\n",
    "    y_input = [np.array([\n",
    "                np.float32(y_input.iloc[i].values)\n",
    "            ])\n",
    "        for i in range(y_input.shape[0])]\n",
    "\n",
    "    batchsize = 50\n",
    "\n",
    "    predshell, trueshell = tf.argmax(y_conv, 1), tf.argmax(y_, 1)\n",
    "    for i in range(df['test'].shape[0]):\n",
    "        predval = predshell.eval(feed_dict={\n",
    "                        keep_prob: 1,\n",
    "                        x:  x_input[i],\n",
    "                        y_: y_input[i]\n",
    "                    })\n",
    "        predval = predval[0]\n",
    "        trueval = trueshell.eval(feed_dict={\n",
    "                        keep_prob: 1,\n",
    "                        x:  x_input[i],\n",
    "                        y_: y_input[i]\n",
    "                    })\n",
    "        trueval = trueval[0]\n",
    "        predlist.append(predval)\n",
    "        truelist.append(trueval)\n",
    "    #     print('ROUND[{}] predict: {} ----- true: {}'.format(i, predval, trueval))\n",
    "\n",
    "    # print(\"CNN testing finished\")\n",
    "\n",
    "    record_data['accuracy'][ID] = accuracy_score(truelist, predlist)\n",
    "    record_data['macro-F1'][ID] = f1_score(truelist, predlist, average='macro')\n",
    "    record_data['micro-F1'][ID] = f1_score(truelist, predlist, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
