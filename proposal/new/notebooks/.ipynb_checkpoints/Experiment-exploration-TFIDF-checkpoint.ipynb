{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化 BOW：原始数据读入read data and save it in data['nearRaw.train'] 和 data['nearRaw.test']\n",
    "if ('init' == phase):# and ('BOW' == idpart['model.word']):\n",
    "    data = {}\n",
    "    data['nearRaw.train'] = {'content':[], 'class':[]}\n",
    "    data['nearRaw.test'] = {'content':[], 'class':[]}\n",
    "\n",
    "    for tpart in ['train', 'test']:\n",
    "        dirpath = paths['dir.{}'.format(tpart)]\n",
    "        for (ind, cls) in enumerate(os.listdir(dirpath)):\n",
    "            clspath = os.path.join(dirpath, cls)\n",
    "            files = os.listdir(clspath)\n",
    "            for f in files:\n",
    "                fpath = os.path.join(clspath, f)\n",
    "                with open(fpath, 'r') as readf:\n",
    "                    data['nearRaw.{}'.format(tpart)]['content'].append(readf.read())\n",
    "                    data['nearRaw.{}'.format(tpart)]['class'].append(cls)\n",
    "        tmp = data['nearRaw.{}'.format(tpart)]\n",
    "        ind = (random.sample(range(len(tmp['class'])), 1))[0]\n",
    "    #     print(\"sample(transformed) from {}[{}]:\\n[content]\\n {}\\n[class]\\n{}\".format(\n",
    "    #             tpart, ind, tmp['content'][ind], tmp['class'][ind]\n",
    "    #         )\n",
    "    #     )\n",
    "    #     print() \n",
    "\n",
    "    # print(\"Step 2 Succeed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立【文档】表示\n",
    "\n",
    "if ('modeling.document' == phase):# and ('BOW' == idpart['model.word']):\n",
    "    # Step: CountVectorizer.fit + TfidfVectorizer.transform + save in Pandas.DataFrame\n",
    "    # A. 使用 sklearn.feature_extraction.text.TfidfVectorizer.fit 拟合训练数据，建立 BOW+TF-IDF \n",
    "    # B. 使用 sklearn.feature_extraction.text.TfidfVectorizer.transform 将data['nearRaw.train']中的 stringContent 和 data['nearRaw.test']中的 stringContent 进行处理，\n",
    "    # 将 BOW+TF-IDF 表示结果输出到 data['matrix.train'] 与 data['matrix.test'] 中，供后续学习和训练使用\n",
    "    # \n",
    "    # C. 将 data['matrix.train'] 与 data['matrix.test'] 转换成 Pandas.DataFrame 格式，保存到 df['train'] 和 df['test'] 中（df 为字典格式：String -> DataFrame）\n",
    "    \n",
    "    # Substep A: vectorization + TF-IDF calculation\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "    if 'NO' == idpart['useStopwords']:\n",
    "        countVector = CountVectorizer(analyzer='word', max_features=vecsize)\n",
    "    else:\n",
    "        import nltk\n",
    "        wordset = []        \n",
    "        for tpart in ['dir.train', 'dir.test']:\n",
    "            dirpath = paths[tpart]\n",
    "            for cls in os.listdir(dirpath):\n",
    "                clspath = os.path.join(dirpath, cls)\n",
    "                files = os.listdir(clspath)\n",
    "                for f in files:\n",
    "                    fpath = os.path.join(clspath, f)\n",
    "                    with open(fpath, 'r', encoding=\"latin-1\") as fpath_f:\n",
    "                        tmpFileStr = fpath_f.read()\n",
    "                    tmpFileStr = tmpFileStr.split()\n",
    "                    wordset += tmpFileStr\n",
    "\n",
    "        wordDist = nltk.FreqDist(wordset)\n",
    "        freqword = [i[0]for i in wordDist.most_common(30)]\n",
    "        countVector = CountVectorizer(analyzer='word', max_features=vecsize, stop_words=freqword)\n",
    "    countVector.fit(data['nearRaw.train']['content'])\n",
    "    tfidf = TfidfTransformer()\n",
    "\n",
    "#     print(\"Substep A finished.\")\n",
    "#     print(\"--------------------------------------------------\")\n",
    "\n",
    "    ## Substep B: Transformation\n",
    "\n",
    "    for tpart in ['train', 'test']:\n",
    "        data['matrix.{}'.format(tpart)] = countVector.transform(data['nearRaw.{}'.format(tpart)]['content']) \n",
    "        data['matrix.{}'.format(tpart)] = tfidf.fit_transform(data['matrix.{}'.format(tpart)])\n",
    "        ind = (random.sample(range(data['matrix.{}'.format(tpart)].shape[0]), 1))[0]\n",
    "#         print(\"sample for matrix.{}\".format(tpart))\n",
    "#         print(\"from ind: {}\".format(ind))\n",
    "#         print(data['matrix.{}'.format(tpart)][ind])\n",
    "#         print() \n",
    "\n",
    "#     print(\"Substep B finished.\")\n",
    "#     print(\"--------------------------------------------------\")\n",
    "\n",
    "    # Substep C: integrate data into DataFrame format\n",
    "\n",
    "#     csvpath_root = os.path.join(paths['dir.dataroot'], 'data_CSV')\n",
    "#     if not os.path.isdir(csvpath_root):\n",
    "#         os.mkdir(csvpath_root)\n",
    "\n",
    "    df = {}\n",
    "    for tpart in ['train', 'test']:\n",
    "        datadict = {}\n",
    "        datadict['class'] = data['nearRaw.{}'.format(tpart)]['class']\n",
    "        for col in range(data['matrix.{}'.format(tpart)].shape[1]):\n",
    "            datadict[col]= [i[0] for i in data['matrix.{}'.format(tpart)].getcol(col).toarray()]\n",
    "    #         datadict[str(col)]= [i[0] for i in data['matrix.{}'.format(tpart)].getcol(col).toarray()]\n",
    "\n",
    "        df[tpart] = pd.DataFrame(data=datadict)\n",
    "        # Shuffle the dataset\n",
    "        df[tpart] = df[tpart].sample(frac=1)\n",
    "        df[tpart].reset_index(drop=True, inplace=True)\n",
    "#         print(\"See df[{}]\".format(tpart))\n",
    "        display(df[tpart])\n",
    "#         print(\"\\n\\n\\n\")\n",
    "#         #write data in DataFrame into CSV\n",
    "#         csvpath = os.path.join(csvpath_root, \"{}-{}-{}.csv\".format(tpart, idpart['model.word'], idpart['model.document']))\n",
    "#         df[tpart].to_csv(csvpath, columns=df[tpart].columns)\n",
    "\n",
    "#     print(\"Substep C finished.\")\n",
    "#     print(\"--------------------------------------------------\")\n",
    "\n",
    "#     print(\"Step 3 Succeed.\")\n",
    "\n",
    "    # 繁琐点：研究如何把 CSR 矩阵中的数据规整好放到 DataFrame 中，并与 Class 一一对应\n",
    "    \n",
    "    # Save in train\n",
    "    X_train, y_train = df['train'].drop('class', axis=1), df['train']['class']\n",
    "    \n",
    "    # Save in test\n",
    "    X_test, y_test_true = df['test'].drop('class', axis=1), df['test']['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if ('modeling.document' == phase) :#and  ('TF*IDF-LSA' == idpart['model.document']):\n",
    "#     # Step: LSA based on BOW+TFIDF\n",
    "\n",
    "#     from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#     svd = TruncatedSVD(n_components=200, n_iter=10, random_state=19)\n",
    "#     svd.fit(df['train'].drop('class', axis=1))\n",
    "\n",
    "#     df_new = {}\n",
    "#     for tpart in ['train', 'test']:\n",
    "#         df_new[tpart] = {}\n",
    "#         datadict = {}\n",
    "#         X_LSA = svd.transform(df[tpart].drop('class', axis=1))\n",
    "#         for col in range(X_LSA.shape[1]):\n",
    "#             datadict[col] = X_LSA[:, col]\n",
    "#         df_new[tpart]['y'] = df[tpart]['class']\n",
    "#         df_new[tpart]['x'] = pd.DataFrame(data=datadict)\n",
    "#         df_new[tpart]['all'] = df_new[tpart]['x'].join(df_new[tpart]['y'])\n",
    "#         display(df_new[tpart]['all'])\n",
    "\n",
    "#     # Save in train\n",
    "#     X_train, y_train = df_new['train']['x'], df_new['train']['y']\n",
    "\n",
    "#     # Save in test\n",
    "#     X_test, y_test_true = df_new['test']['x'], df_new['test']['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if ('modeling.document' == phase) and  ('TF*IDF-LDA' == idpart['model.document']):\n",
    "#     # Step: LDA based on BOW+TFIDF\n",
    "\n",
    "#     from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#     lda = LatentDirichletAllocation(n_topics=50, max_iter=10, random_state=19, learning_method='batch')\n",
    "#     lda.fit(df['train'].drop('class', axis=1))\n",
    "\n",
    "#     df_new = {}\n",
    "#     for tpart in ['train', 'test']:\n",
    "#         df_new[tpart] = {}\n",
    "#         datadict = {}\n",
    "#         X_LDA = lda.transform(df[tpart].drop('class', axis=1))\n",
    "#         for col in range(X_LDA.shape[1]):\n",
    "#             datadict[col] = X_LDA[:, col]\n",
    "#         df_new[tpart]['y'] = df[tpart]['class']\n",
    "#         df_new[tpart]['x'] = pd.DataFrame(data=datadict)\n",
    "#         df_new[tpart]['all'] = df_new[tpart]['x'].join(df_new[tpart]['y'])\n",
    "#         display(df_new[tpart]['all'])\n",
    "        \n",
    "#     # Save in train\n",
    "#     X_train, y_train = df_new['train']['x'], df_new['train']['y']\n",
    "\n",
    "#     # Save in test\n",
    "#     X_test, y_test_true = df_new['test']['x'], df_new['test']['y']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
