{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化 Word2Vec：\n",
    "\n",
    "if ('init' == phase):\n",
    "    # 1. 定义模型保存位置\n",
    "    paths['dir.modelroot'] = os.path.join(paths['dir.dataroot'], '..', 'models')\n",
    "    for modeltool in ['gensim']:\n",
    "        for embedsource in ['corpus']:\n",
    "            dname = os.path.join(paths['dir.modelroot'], '{}.{}'.format(modeltool, embedsource))\n",
    "            if not os.path.isdir(dname):\n",
    "                os.mkdir(dname)\n",
    "            paths['dir.{}.{}'.format(modeltool, embedsource)] = dname\n",
    "      \n",
    "    # 2. 读入停止词表\n",
    "    stopwords = \"\"\n",
    "\n",
    "    pathtemp_TFIDF = os.path.join(paths['dir.dataroot'], 'stoplist-baseTFIDF.txt')\n",
    "    with open(pathtemp_TFIDF, 'r') as stoplistfile:\n",
    "        stopwords = stoplistfile.read()\n",
    "    stopwords = stopwords.split()\n",
    "\n",
    "#     pathtemp_web = os.path.join(paths['dir.dataroot'], 'stoplist-web.txt')\n",
    "#     with open(pathtemp_web, 'r') as stoplistfile2:\n",
    "#         stopwords2 = stoplistfile2.read()\n",
    "#         stopwords2 = stopwords2.split('\\n')\n",
    "#         stopwords = set(stopwords)\n",
    "#         stopwords = list(stopwords.union(set(stopwords)))\n",
    "\n",
    "#     print(\"Read stop words successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立【词】表示\n",
    "\n",
    "if ('modeling.document' == phase):\n",
    "    count_clusters = vecsize\n",
    "    \n",
    "    data = {}\n",
    "    data['nearRaw.train'] = {'corpus':[], 'class':[]}\n",
    "    data['nearRaw.test'] = {'corpus':[], 'class':[]}\n",
    "    \n",
    "    ## read and save data\n",
    "\n",
    "    for tpart in ['train', 'test']:\n",
    "        dirpath = paths['dir.{}'.format(tpart)]\n",
    "        for (ind, cls) in enumerate(os.listdir(dirpath)):\n",
    "            clspath = os.path.join(dirpath, cls)\n",
    "            files = os.listdir(clspath)\n",
    "            for f in files:\n",
    "                fpath = os.path.join(clspath, f)\n",
    "                with open(fpath, 'r') as readf:\n",
    "                    tokens = [token for token in readf.read().split()]#readf.read().split()#\n",
    "                    if 'train' == tpart:\n",
    "                        corpus = gensim.models.doc2vec.TaggedDocument(tokens, [cls])\n",
    "                    else:\n",
    "                        corpus = tokens\n",
    "                    data['nearRaw.{}'.format(tpart)]['corpus'].append(corpus)\n",
    "                    data['nearRaw.{}'.format(tpart)]['class'].append(cls)\n",
    "\n",
    "                    \n",
    "    # Training the Model\n",
    "    ## Substep: Initialization\n",
    "    vocabularySize = vecsize\n",
    "    model = gensim.models.doc2vec.Doc2Vec(size=vocabularySize, iter=250, workers=4, alpha=0.025, min_alpha=0.005)\n",
    "    \n",
    "    # Set trimmer\n",
    "    if 'NO' == idpart['useStopwords']:\n",
    "        trimmer = lambda word, count, min_count: gensim.utils.RULE_KEEP\n",
    "    else:\n",
    "        trimmer = lambda word, count, min_count: gensim.utils.RULE_DISCARD if word in stopwords else gensim.utils.RULE_KEEP\n",
    "    \n",
    "    # Build vocabulary\n",
    "    model.build_vocab(data['nearRaw.train']['corpus'])\n",
    "    \n",
    "    # Train model of word\n",
    "    model.train(sentences=data['nearRaw.train']['corpus'], total_examples=model.corpus_count, epochs=model.iter)\n",
    "    \n",
    "    # save model\n",
    "    calander = datetime.date.today().timetuple()\n",
    "    modelpath = os.path.join(\n",
    "        paths['dir.{}.{}'.format(idpart['tool.word'], idpart['model.source'])],\n",
    "        '{}.{}.{}{}{}'.format(idpart['tool.word'], idpart['model.source'], calander.tm_year, calander.tm_mon, calander.tm_mday)\n",
    "    )\n",
    "    model.save(modelpath)\n",
    "    \n",
    "    # Substep: Infer vectors for posts\n",
    "    for tpart in ['train', 'test']:\n",
    "        countCorpus = len(data['nearRaw.{}'.format(tpart)]['corpus'])\n",
    "        data['matrix.{}'.format(tpart)] = []\n",
    "        for ind in range(countCorpus):\n",
    "            if tpart == 'train':\n",
    "                data['matrix.{}'.format(tpart)].append(model.infer_vector(data['nearRaw.{}'.format(tpart)]['corpus'][ind][0]))\n",
    "            else:\n",
    "                data['matrix.{}'.format(tpart)].append(model.infer_vector(data['nearRaw.{}'.format(tpart)]['corpus'][ind]))\n",
    "                \n",
    "    ## Step: save into DataFrame\n",
    "    df = {}\n",
    "    for tpart in ['train', 'test']:\n",
    "        datadict = {}\n",
    "        datadict['class'] = data['nearRaw.{}'.format(tpart)]['class']\n",
    "        for feature_ind in range(vocabularySize):#range(len(data['matrix.{}'.format(tpart)][0])):#\n",
    "            m = np.array(data['matrix.{}'.format(tpart)])\n",
    "            datadict[feature_ind] = m[:,feature_ind]\n",
    "\n",
    "        df[tpart] = pd.DataFrame(data=datadict)\n",
    "        df[tpart] = df[tpart].sample(frac=1)\n",
    "        df[tpart].reset_index(drop=True, inplace=True)\n",
    "        display(df[tpart])\n",
    "        \n",
    "    # Store data in X_train, y_train, X_test, y_test\n",
    "    ##train\n",
    "    X_train, y_train = df['train'].drop('class', axis=1), df['train']['class']\n",
    "    ##test\n",
    "    X_test, y_test_true = df['test'].drop('class', axis=1), df['test']['class']\n",
    "    \n",
    "    # If or not transform into TFIDF\n",
    "    if 'BagOfConcepts-TFIDF' == idpart['model.document']:\n",
    "        from sklearn.feature_extraction.text import TfidfTransformer\n",
    "        tfidf = TfidfTransformer()\n",
    "        #train\n",
    "        X_train = tfidf.fit_transform(X_train)\n",
    "        #test\n",
    "        X_test = tfidf.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
