{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练前制作 ID，保存数据\n",
    "\n",
    "if 'save_before_train' == phase:\n",
    "    ID = '{}.{}.{}'.format(idpart['tool.word'], idpart['useStopwords'], idpart['classifier'])\n",
    "    record_data[ID] = {}\n",
    "    for ilabel in idpart:\n",
    "        record_data[ilabel][ID] = idpart[ilabel]\n",
    "\n",
    "    for labelpart in ['raw', 'format']:\n",
    "        for ilabel in ['train', 'evaluate', 'all']:\n",
    "            record_data['time.{}.{}'.format(labelpart, ilabel)][ID] = {} if 'raw' == labelpart else 0.0\n",
    "\n",
    "#         for timepart, timesvm in record_data['time.raw.model'][ID_SVM].items():\n",
    "#             record_data['time.raw.model'][ID][timepart] = timesvm + timev[timepart]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 实验后处理：显示数据\n",
    "\n",
    "if 'post-display' == phase:\n",
    "    # sum of time\n",
    "\n",
    "    for i in timeDict.keys():\n",
    "        record_data['time.raw.all'][ID][i] = sum([record_data['time.raw.{}'.format(timepart)][ID][i] for timepart in ['train', 'evaluate']])\n",
    "\n",
    "    # formation of time\n",
    "\n",
    "    for timepart in ['train', 'evaluate', 'all']:\n",
    "            record_data['time.format.{}'.format(timepart)][ID] = timeInFormat(record_data['time.raw.{}'.format(timepart)][ID])\n",
    "\n",
    "#     print(record_data)\n",
    "#     tmptime = record_data['time.raw.all'][ID]['h'] * 60 * 60 + record_data['time.raw.all'][ID]['min'] * 60 + \\\n",
    "#                 record_data['time.raw.all'][ID]['sec'] + record_data['time.raw.all'][ID]['ms'] / 1e3 + \\\n",
    "#                 record_data['time.raw.all'][ID]['us'] / 1e6 \n",
    "#     record_data['score-macro-F1'][ID] = record_data['macro-F1'][ID] / tmptime\n",
    "#     record_data['score-micro-F1'][ID] = record_data['micro-F1'][ID] / tmptime\n",
    "\n",
    "    records_df_raw = pd.DataFrame(\n",
    "        data=record_data, \n",
    "        columns=record_labels\n",
    "    )\n",
    "\n",
    "    # remove raw time\n",
    "    records_df_display = records_df_raw.drop(record_labels[-4:], axis=1)\n",
    "\n",
    "    # use reset_index(drop=True) to use integers instead of ID as the indices\n",
    "    # (data can be distinguish using each column, so ID is dispensable)\n",
    "    records_df_display = records_df_display.reset_index(drop=True)\n",
    "\n",
    "    display(records_df_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot representation for labels\n",
    "# 标签独热向量化\n",
    "\n",
    "if 'one-hot-labels' == phase:\n",
    "#     csvpath_root = os.path.join(paths['dir.dataroot'], 'data_CSV')\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(df['train']['class'])\n",
    "\n",
    "    df_new = {}\n",
    "    for tpart in ['train', 'test']:\n",
    "        labels = lb.transform(df[tpart]['class'])\n",
    "        labelsDf = pd.DataFrame(labels, columns=[\"class-{}\".format(i) for i in range(len(lb.classes_))])\n",
    "        df_new[tpart] = {}\n",
    "        df_new[tpart]['y'] = labelsDf\n",
    "        df_new[tpart]['x'] = df[tpart].drop('class', axis=1)\n",
    "        df_new[tpart]['all'] = df_new[tpart]['x'].join(df_new[tpart]['y'])\n",
    "        #save in CSV\n",
    "#         for subpart in ['x', 'y', 'all']:\n",
    "#             csvpath = os.path.join(csvpath_root, '{}-cleanLabels{}-{}-{}-{}.csv'.format(tpart, idpart['model.source'], idpart['tool.word'], idpart['model.word'], idpart['model.document']))\n",
    "#             df_new[tpart][subpart].to_csv(csvpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN train\n",
    "# 训练分类器：DNN\n",
    "if 'DNN-train' == phase: \n",
    "    COL_OUTCOME = 'class'\n",
    "    COL_FEATURE = [str(col) for col in list(df['train'].columns) if col != COL_OUTCOME]\n",
    "\n",
    "    cls2num = {cls:ind for (ind, cls) in enumerate(df['train']['class'].unique())}\n",
    "\n",
    "    def my_input_fn(dataset):\n",
    "        # Save dataset in tf format\n",
    "        feature_cols = {\n",
    "            str(col): tf.constant(\n",
    "                df[dataset][int(col)].values#[str(col)].values\n",
    "            )\n",
    "            for col in COL_FEATURE\n",
    "        }\n",
    "        labels = tf.constant([cls2num[labelname] for labelname in df[dataset][COL_OUTCOME].values])\n",
    "        # Returns the feature columns and labels in tf format\n",
    "        return feature_cols, labels\n",
    "\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(column_name=str(col)) for col in COL_FEATURE]\n",
    "    clf = tf.contrib.learn.DNNClassifier(\n",
    "        feature_columns=feature_columns, \n",
    "        hidden_units=[512], \n",
    "        n_classes=len(df['train']['class'].unique())\n",
    "    )\n",
    "\n",
    "    clf.fit(input_fn=lambda: my_input_fn('train'), steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN evaluate\n",
    "# 评估分类器：DNN\n",
    "if 'DNN-evaluate' == phase: \n",
    "    X_tensor_test, yt = my_input_fn('test')\n",
    "    tensorPredCls = list(clf.predict(input_fn=lambda: my_input_fn('test')))\n",
    "    num2cls = {v:k for (k, v) in cls2num.items()}\n",
    "    tensorPredClsStr = [num2cls[i] for i in tensorPredCls]\n",
    "    y_test_true = df['test']['class']\n",
    "\n",
    "    record_data['accuracy'][ID] = accuracy_score(y_test_true, tensorPredClsStr)\n",
    "    record_data['macro-F1'][ID] = f1_score(y_test_true, tensorPredClsStr, average='macro')\n",
    "    record_data['micro-F1'][ID] = f1_score(y_test_true, tensorPredClsStr, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN-init\n",
    "# 训练前初始化：CNN\n",
    "if 'CNN-init' == phase:\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    COL_OUTCOME = 'class'\n",
    "    COL_FEATURE = [col for col in list(df['train'].columns) if col != COL_OUTCOME]\n",
    "\n",
    "    # cls2num = {cls:ind for (ind, cls) in enumerate(df['train']['class'].unique())}\n",
    "\n",
    "    count_feature = len(COL_FEATURE)\n",
    "    count_class = len(df['train']['class'].unique())\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, count_clusters], name='x')\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, count_class], name='y_')\n",
    "\n",
    "    W = tf.Variable(tf.zeros([count_feature, count_class]), name='W')\n",
    "    b = tf.Variable(tf.zeros([count_class]), name='b')\n",
    "    y = tf.matmul(x, W, name='y') + b\n",
    "\n",
    "    # cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "    def weight_variable(shape, name=None):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name)\n",
    "\n",
    "    def bias_variable(shape, name=None):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name)\n",
    "\n",
    "    def conv2d(x, W, name=None):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool_2x2(x, name=None):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32], name='W_conv1')\n",
    "    b_conv1 = bias_variable([32], name='b_conv1')\n",
    "    \n",
    "    if 'ArithmeticMean' == idpart['model.document'] or ('Doc2Vec' == idpart['model.document']):    \n",
    "        x_text = tf.reshape(x, [-1, 28, 28, 1], name='x_text')\n",
    "    else:\n",
    "        x_text = tf.reshape(x, [-1, 14, 14, 1], name='x_text')\n",
    "        \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_text, W_conv1) + b_conv1, name='h_conv1')\n",
    "    h_pool1 = max_pool_2x2(h_conv1, name='h_pool1')\n",
    "\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64], name='W_conv2')\n",
    "    b_conv2 = bias_variable([64], name='b_conv2')\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2, name='h_conv2')\n",
    "    h_pool2 = max_pool_2x2(h_conv2, name='h_pool2')\n",
    "\n",
    "    if ('ArithmeticMean' == idpart['model.document']) or ('Doc2Vec' == idpart['model.document']):    \n",
    "        W_fc1 = weight_variable([7 * 7 * 64, 1024], name='W_fc1')\n",
    "    else:\n",
    "        W_fc1 = weight_variable([4 * 4 * 64, 1024], name='W_fc1')\n",
    "    b_fc1 = bias_variable([1024], name='b_fc1')\n",
    "\n",
    "    if ('ArithmeticMean' == idpart['model.document']) or ('Doc2Vec' == idpart['model.document']):\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64], name='h_pool2_flat')\n",
    "    else:\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 4 * 4 * 64], name='h_pool2_flat')\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name='h_fc1')\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob, name='h_fc1_drop')\n",
    "\n",
    "    W_fc2 = weight_variable([1024, count_class], name='W_fc2')\n",
    "    b_fc2 = bias_variable([count_class], name='b_fc2')\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2, name='y_conv') + b_fc2\n",
    "\n",
    "    # print(\"CNN initialization finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN train\n",
    "# 训练分类器：CNN\n",
    "if 'CNN-train' == phase: \n",
    "    ### Start to train and evaluate the model\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv), name='cross_entropy')\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    # correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    x_input = df_new['train']['x']\n",
    "    x_input = [np.array([\n",
    "                np.float32(x_input.iloc[i].values)\n",
    "            ])\n",
    "        for i in range(x_input.shape[0])]\n",
    "    y_input = df_new['train']['y']\n",
    "    y_input = [np.array([\n",
    "                np.float32(y_input.iloc[i].values)\n",
    "            ])\n",
    "        for i in range(y_input.shape[0])]\n",
    "    # y_input = [np.array([y_input.iloc[i].values]) for i in range(y_input.shape[0])]\n",
    "\n",
    "    # not use random input\n",
    "\n",
    "    batchsize = 50\n",
    "    for i in range(df['train'].shape[0] - batchsize):\n",
    "    #     if 0 == i % 100:\n",
    "    #         train_accuracy = []\n",
    "    #         for j in range(50):\n",
    "    #             train_accuracy.append(accuracy.eval(feed_dict={\n",
    "    #                     keep_prob: 1,\n",
    "    #                     x:  np.array([elem[0] for elem in x_input[i+j:i+j+50]]),#x_input.iloc[i+j].values, #\n",
    "    #                     y_: np.array([elem[0] for elem in y_input[i+j:i+j+50]])#y_input.iloc[i+j].values #\n",
    "    #                 })\n",
    "    #             )\n",
    "    #         print(\"step {}, training accuracy {}\".format(i, np.mean(train_accuracy)))\n",
    "        train_step.run(feed_dict={\n",
    "            keep_prob: 0.5,\n",
    "            x:  np.array([elem[0] for elem in x_input[i:i+batchsize]]),#x_input.iloc[i].values, #\n",
    "            y_: np.array([elem[0] for elem in y_input[i:i+batchsize]])#y_input.iloc[i].values#\n",
    "        })\n",
    "\n",
    "    # print(\"CNN training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN evaluate\n",
    "# 评估分类器：CNN\n",
    "if 'CNN-evaluate' == phase: \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    # Evaluate\n",
    "\n",
    "    tfacc = []\n",
    "    predlist, truelist = [], []\n",
    "\n",
    "    x_input = df_new['test']['x']\n",
    "    x_input = [np.array([\n",
    "                np.float32(x_input.iloc[i].values)\n",
    "            ])\n",
    "        for i in range(x_input.shape[0])]\n",
    "    y_input = df_new['test']['y']\n",
    "    y_input = [np.array([\n",
    "                np.float32(y_input.iloc[i].values)\n",
    "            ])\n",
    "        for i in range(y_input.shape[0])]\n",
    "\n",
    "    batchsize = 50\n",
    "\n",
    "    predshell, trueshell = tf.argmax(y_conv, 1), tf.argmax(y_, 1)\n",
    "    for i in range(df['test'].shape[0]):\n",
    "        predval = predshell.eval(feed_dict={\n",
    "                        keep_prob: 1,\n",
    "                        x:  x_input[i],\n",
    "                        y_: y_input[i]\n",
    "                    })\n",
    "        predval = predval[0]\n",
    "        trueval = trueshell.eval(feed_dict={\n",
    "                        keep_prob: 1,\n",
    "                        x:  x_input[i],\n",
    "                        y_: y_input[i]\n",
    "                    })\n",
    "        trueval = trueval[0]\n",
    "        predlist.append(predval)\n",
    "        truelist.append(trueval)\n",
    "    #     print('ROUND[{}] predict: {} ----- true: {}'.format(i, predval, trueval))\n",
    "\n",
    "    # print(\"CNN testing finished\")\n",
    "\n",
    "    record_data['accuracy'][ID] = accuracy_score(truelist, predlist)\n",
    "    record_data['macro-F1'][ID] = f1_score(truelist, predlist, average='macro')\n",
    "    record_data['micro-F1'][ID] = f1_score(truelist, predlist, average='micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
