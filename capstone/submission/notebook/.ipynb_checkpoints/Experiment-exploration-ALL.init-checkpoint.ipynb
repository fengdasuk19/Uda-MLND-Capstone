{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模块导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import datetime\n",
    "import copy\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "import gensim\n",
    "# import tensorflow as tf\n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\\nImport modules successfully\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 由 %%capture captObj 配合 %%time 所获得的时间 captObj 中抽取所需的时间参数\n",
    "\n",
    "def getCellTime_Wall(captObj):\n",
    "    \"\"\"\n",
    "    Extract h, min, sec, ms from captObj gotton by %%capture and %%time\n",
    "    ----------------\n",
    "    Arguments: \n",
    "    \n",
    "        captObj  `IPython.utils.capture.CapturedIO`, contains string captured using magic command %%capature\n",
    "        \n",
    "    ----------------\n",
    "    Returns:\n",
    "    \n",
    "        rectime  a dictionary, containing time extracted from captObj\n",
    "    \"\"\"\n",
    "    # Ref\n",
    "    # https://docs.python.org/3/library/re.html\n",
    "    # https://docs.python.org/3/howto/regex.html#regex-howto\n",
    "\n",
    "    m = re.search('(?<=Wall time: ).*[^\\n]', captObj.stdout)\n",
    "    mstr = m.group(0)\n",
    "    rectime = {}\n",
    "    rectime['h'] = re.search('(\\d+\\.?\\d*)(?= ?h)', mstr)\n",
    "    rectime['min'] = re.search('(\\d+\\.?\\d*)(?= ?min)', mstr)\n",
    "    rectime['sec'] = re.search('(\\d+\\.?\\d*)(?= ?s)', mstr)\n",
    "    rectime['ms'] = re.search('(\\d+\\.?\\d*)(?= ?ms)', mstr)\n",
    "    rectime['us'] = re.search('(\\d+\\.?\\d*)(?= ?us)', mstr)\n",
    "\n",
    "    for ptn in ['h', 'min', 'sec', 'ms', 'us']:\n",
    "        rectime[ptn] = 0 if rectime[ptn] is None else float(rectime[ptn].group(0))\n",
    "        \n",
    "    print(rectime)\n",
    "    print('Record time successfully.')\n",
    "    \n",
    "    return rectime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeInFormat(timedict):\n",
    "    t_h, t_min, t_s, t_ms, t_us = timedict['h'], timedict['min'], timedict['sec'], timedict['ms'], timedict['us']\n",
    "    return str(datetime.timedelta(seconds=t_s, microseconds=t_us, milliseconds=t_ms, minutes=t_min, hours=t_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import Word\n",
    "from textblob import Blobber\n",
    "from textblob import TextBlob\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_footer, strip_newsgroup_quoting\n",
    "\n",
    "num2word = {'0':'zero', '1':'one', '2':'two', '3':'three', '4':'four',\n",
    "            '5':'five', '6':'six', '7':'seven', '8':'eight', '9':'night'}\n",
    "\n",
    "def dealPunctNumLemma(text):\n",
    "    lowerText = text.lower()\n",
    "    tb = Blobber()\n",
    "    blob_01 = tb(lowerText)\n",
    "    \n",
    "    processed = \"\"\n",
    "    for sentence in blob_01.sentences:\n",
    "        #remove punctuation\n",
    "        nopunct_string = \"\"\n",
    "        newtext = sentence.raw\n",
    "        newtext = re.sub(r'[^a-zA-Z0-9_]', r' ', newtext) #remove punctuation\n",
    "        newtext = re.sub(r'\\s{2,}', r' ', newtext) #remove continuous spaces\n",
    "        newtext = ' '.join([i for i in newtext.split() if not((1 == len(i)) and i.isalpha() and ('a' != i))]) #remove single alpha(nonsense)\n",
    "        newtext = ' '.join([' '.join([num2word[i] for i in iStr]) if iStr.isnumeric() else iStr \n",
    "                            for iStr in newtext.split()]) # represent digit in word form\n",
    "        nopunct_string = newtext\n",
    "        \n",
    "        # remove the period signal\n",
    "        if (len(nopunct_string) >= 1) and ('.' == nopunct_string[-1]):\n",
    "            nopunct_string = nopunct_string[:-1]\n",
    "        nopunct_sentence = TextBlob(nopunct_string)\n",
    "        \n",
    "        #lemmatization\n",
    "        vocabs, tags = [], []\n",
    "        for ivocab, itag in nopunct_sentence.tags:\n",
    "            vocabs.append(Word(ivocab))\n",
    "            tags.append(itag)\n",
    "        newStr = \"\"\n",
    "        for ivocab, itag in zip(vocabs, tags):\n",
    "            try:\n",
    "                newStr += ivocab.lemmatize(itag[0].lower())\n",
    "            except:\n",
    "                newStr += ivocab.lemmatize()\n",
    "            newStr += ' '\n",
    "        processed += \"{}\\n\".format(newStr)\n",
    "        \n",
    "    return processed\n",
    "\n",
    "_QUOTE_RE = re.compile(r'(writes in|writes:|wrote:|says:|said:'\n",
    "                       r'|^In article|^Quoted from)') #|^\\||^>)')\n",
    "\n",
    "def custom_quoting(rawtext):\n",
    "    text = re.sub(r'(^\\||^>)', ' ', rawtext)\n",
    "    good_lines = [line for line in text.split('\\n')\n",
    "                  if not _QUOTE_RE.search(line)]\n",
    "    return '\\n'.join(good_lines)\n",
    "\n",
    "def cleanText(rawstring):\n",
    "    textHead, _blankline, textBody = rawstring.partition('\\n\\n') # ref: strip_newsgroup_header\n",
    "    textCore = custom_quoting(textBody) # ref: strip_newsgroup_quoting\n",
    "    lowerCoreList = textCore.split('\\n')\n",
    "    lowerCore = \" \".join(lowerCoreList)\n",
    "\n",
    "    heads = [ihead for ihead in textHead.split('\\n') if not ihead.startswith('Lines:')]\n",
    "    fileStr = \"! \".join(heads)\n",
    "    fileStr = dealPunctNumLemma(fileStr)\n",
    "    fileStr += '\\n'\n",
    "    fileStr += dealPunctNumLemma(lowerCore)\n",
    "        \n",
    "    return fileStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 初始化即将用于记录数据的数据结构\n",
    "\n",
    "idpart = {}\n",
    "record_data = {}\n",
    "record_labels = ['tool.word', 'useStopwords', 'classifier', 'micro-F1'] #'accuracy', 'macro-F1', \n",
    "record_labels += ['time.{}.{}'.format(labelpart, ilabel) for labelpart in ['format', 'raw'] for ilabel in ['train', 'evaluate', 'all']]\n",
    "for record_label in record_labels:\n",
    "    record_data[record_label] = {}\n",
    "    \n",
    "print('Create:\\nidpart:{} \\n record_data:{}\\n record_labels:{}'.format(idpart, record_data, record_labels))\n",
    "print('\\nData structure for recording experiment data created.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 文本清洗\n",
    "\n",
    "# vecsize: vector dimension\n",
    "# casesize: how many groups will be used in the classification\n",
    "vecsize = 500\n",
    "casesize = 20\n",
    "\n",
    "# paths: dict of some important path\n",
    "paths = {}\n",
    "paths['dir.dataroot'] =  os.path.join(os.getcwd(), '..', 'data')\n",
    "paths['dir.train'] = os.path.join(paths['dir.dataroot'], 'trialdata', 'train')\n",
    "paths['dir.test'] = os.path.join(paths['dir.dataroot'], 'trialdata', 'test')\n",
    "        \n",
    "# if source data dosen't exist in target directory, copy them from the source data directory\n",
    "existedFlag = os.path.join(paths['dir.dataroot'], 'existedFlag')\n",
    "if not os.path.isfile(existedFlag):\n",
    "    paths['dir.src_dataroot'] =  os.path.join(paths['dir.dataroot'], 'srcdata', '20news-bydate')\n",
    "    paths['dir.src_train'] =  os.path.join(paths['dir.src_dataroot'], '20news-bydate-train')\n",
    "    paths['dir.src_test'] =  os.path.join(paths['dir.src_dataroot'], '20news-bydate-test')\n",
    "    dirs_all = os.listdir(paths['dir.src_train'])\n",
    "    dirs_randInds = random.sample(range(len(dirs_all)), casesize)\n",
    "    dirs_rands = [dirs_all[i] for i in dirs_randInds]\n",
    "    for tpart in ['train', 'test']:\n",
    "        for dname in dirs_rands:\n",
    "            dpath = os.path.join(paths['dir.src_{}'.format(tpart)], dname)\n",
    "            shutil.copytree(dpath,  os.path.join(paths['dir.{}'.format(tpart)], dname))\n",
    "    os.system('touch {}'.format(existedFlag))\n",
    "\n",
    "print(\"Pick random folders from source successfully\")\n",
    "\n",
    "preprocessedFlag = os.path.join(paths['dir.dataroot'], 'preprocessed')\n",
    "if not os.path.isfile(preprocessedFlag):\n",
    "    for tpart in ['train', 'test']:\n",
    "        dirpath = paths['dir.{}'.format(tpart)]\n",
    "        for cls in os.listdir(dirpath):\n",
    "            clspath = os.path.join(dirpath, cls)\n",
    "            files = os.listdir(clspath)\n",
    "            for eachfile in files:\n",
    "                fpath = os.path.join(clspath, eachfile)\n",
    "                with open(fpath, 'r', encoding=\"latin-1\") as f:\n",
    "                    fcontent = f.read()\n",
    "                with open(fpath, 'w') as f:\n",
    "                    newcontent = cleanText(fcontent)\n",
    "                    f.write(newcontent)\n",
    "    os.system('touch {}'.format(preprocessedFlag))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
